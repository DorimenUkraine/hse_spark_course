{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Устройство Spark Dataframe API\n",
    "+ Чтение данных из источника\n",
    "+ Работа с данными\n",
    "  - Базовый SQL\n",
    "  - NA функции\n",
    "  - Группировки\n",
    "  - Запись данных\n",
    "  - Соединения\n",
    "  - Оконные функции\n",
    "  - Функции pyspark.sql.functions\n",
    "  - UDF функции\n",
    "+ Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Файлы с данными\n",
    "json_file = ''\n",
    "output_parquet_agg = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Устройство Spark Dataframe API\n",
    "\n",
    "**Dataframe:**\n",
    "+ структурированная колоночная структура данных\n",
    "+ может быть создана на основе:\n",
    "  - локальной коллекции\n",
    "  - файла (файлов)\n",
    "  - базы данных\n",
    "+ в python работает значительно быстрее, чем RDD\n",
    "+ под капотом использует RDD\n",
    "+ позволяет выполнять произвольные SQL операции с данными\n",
    "+ аналогично RDD являются ленивыми и неизменяеыми\n",
    "\n",
    "## Из чего состоит Dataframe\n",
    "+ схема [pyspsark.sql.StructType](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StructType)\n",
    "+ колонки [pyspark.sql.Column](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column)\n",
    "+ данные [pyspark.sql.Row](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+---------+-------+---------+----------+\n",
      "|_corrupt_record                      |continent|country|name     |population|\n",
      "+-------------------------------------+---------+-------+---------+----------+\n",
      "|null                                 |Europe   |Russia |Moscow   |12380664  |\n",
      "|null                                 |null     |Spain  |Madrid   |null      |\n",
      "|null                                 |Europe   |France |Paris    |2196936   |\n",
      "|null                                 |Europe   |Germany|Berlin   |3490105   |\n",
      "|null                                 |Europe   |Spain  |Barselona|null      |\n",
      "|null                                 |Africa   |Egypt  |Cairo    |11922948  |\n",
      "|null                                 |Africa   |Egypt  |Cairo    |11922948  |\n",
      "|{ \"name\":\"New York, \"country\":\"USA\", |null     |null   |null     |null      |\n",
      "+-------------------------------------+---------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.StructType"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_corrupt_record', 'continent', 'country', 'name', 'population']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|continent|country|\n",
      "+---------+-------+\n",
      "|Europe   |Russia |\n",
      "|null     |Spain  |\n",
      "|Europe   |France |\n",
      "+---------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.select(\n",
    "    col(\"continent\"), col(\"country\")\n",
    ").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_corrupt_record=None, continent='Europe', country='Russia', name='Moscow', population=12380664),\n",
       " Row(_corrupt_record=None, continent=None, country='Spain', name='Madrid', population=None),\n",
       " Row(_corrupt_record=None, continent='Europe', country='France', name='Paris', population=2196936),\n",
       " Row(_corrupt_record=None, continent='Europe', country='Germany', name='Berlin', population=3490105),\n",
       " Row(_corrupt_record=None, continent='Europe', country='Spain', name='Barselona', population=None),\n",
       " Row(_corrupt_record=None, continent='Africa', country='Egypt', name='Cairo', population=11922948),\n",
       " Row(_corrupt_record=None, continent='Africa', country='Egypt', name='Cairo', population=11922948),\n",
       " Row(_corrupt_record='{ \"name\":\"New York, \"country\":\"USA\", ', continent=None, country=None, name=None, population=None)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чтение данных из источника\n",
    "Основной метод чтения любых источников\n",
    "\n",
    "```df = spark.read.format(datasource_type).option(datasource_options).load(object_name)```\n",
    "\n",
    "+ ```datasource_type``` - тип источника (\"parquet\", \"json\", \"cassandra\") и т. д.\n",
    "+ ```datasource_options``` - опции для работы с источником (логины, пароли, адреса для подключения и т. д.)\n",
    "+ ```object_name``` - имя таблицы/файла/топика/индекса\n",
    "\n",
    "[DataframeReader](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader):\n",
    "+ по умолчанию выводит схему данных\n",
    "+ является трансформацией (ленивый)\n",
    "+ возвращает [Dataframe](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    "\n",
    "### Список (неполный) поддерживаемых источников данных\n",
    "+ Файлы:\n",
    "  - json\n",
    "  - text\n",
    "  - csv\n",
    "  - orc\n",
    "  - parquet\n",
    "+ Базы данных\n",
    "  - elasticsearch\n",
    "  - cassandra\n",
    "  - jdbc\n",
    "  - hive\n",
    "+ Брокеры сообщений\n",
    "  - kafka\n",
    "  \n",
    "\n",
    "**Библиотеки для работы с источниками должны быть доступны в JAVA CLASSPATH на драйвере и воркерах!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+---------+-------+---------+----------+\n",
      "|_corrupt_record                      |continent|country|name     |population|\n",
      "+-------------------------------------+---------+-------+---------+----------+\n",
      "|null                                 |Europe   |Russia |Moscow   |12380664  |\n",
      "|null                                 |null     |Spain  |Madrid   |null      |\n",
      "|null                                 |Europe   |France |Paris    |2196936   |\n",
      "|null                                 |Europe   |Germany|Berlin   |3490105   |\n",
      "|null                                 |Europe   |Spain  |Barselona|null      |\n",
      "|null                                 |Africa   |Egypt  |Cairo    |11922948  |\n",
      "|null                                 |Africa   |Egypt  |Cairo    |11922948  |\n",
      "|{ \"name\":\"New York, \"country\":\"USA\", |null     |null   |null     |null      |\n",
      "+-------------------------------------+---------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(json_file)\n",
    "\n",
    "df.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|name     |population|\n",
      "+---------+-------+---------+----------+\n",
      "|Europe   |Russia |Moscow   |12380664  |\n",
      "|null     |Spain  |Madrid   |null      |\n",
      "|Europe   |France |Paris    |2196936   |\n",
      "|Europe   |Germany|Berlin   |3490105   |\n",
      "|Europe   |Spain  |Barselona|null      |\n",
      "|Africa   |Egypt  |Cairo    |11922948  |\n",
      "|Africa   |Egypt  |Cairo    |11922948  |\n",
      "|null     |null   |null     |null      |\n",
      "+---------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Убираем ненужные колонки\n",
    "df.drop(\"_corrupt_record\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|     null|   null|     null|      null|\n",
      "|     null|  Spain|   Madrid|      null|\n",
      "|   Europe|  Spain|Barselona|      null|\n",
      "|   Europe| France|    Paris|   2196936|\n",
      "|   Europe|Germany|   Berlin|   3490105|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|   Europe| Russia|   Moscow|  12380664|\n",
      "+---------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Удаляем дубликаты\n",
    "df \\\n",
    "    .drop(\"_corrupt_record\") \\\n",
    "    .distinct() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|     null|  Spain|   Madrid|      null|\n",
      "|   Europe|  Spain|Barselona|      null|\n",
      "|   Europe| France|    Paris|   2196936|\n",
      "|   Europe|Germany|   Berlin|   3490105|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|   Europe| Russia|   Moscow|  12380664|\n",
      "+---------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Удаляем пустые строки. Параметр \"all\" означает, что будут удалены только те строки, в которых ВСЕ элементы null\n",
    "df \\\n",
    "    .drop(\"_corrupt_record\") \\\n",
    "    .distinct() \\\n",
    "    .na.drop(\"all\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|    Earth|  Spain|   Madrid|         0|\n",
      "|   Europe|  Spain|Barselona|         0|\n",
      "|   Europe| France|    Paris|   2196936|\n",
      "|   Europe|Germany|   Berlin|   3490105|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|   Europe| Russia|   Moscow|  12380664|\n",
      "+---------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Заполняем пустые значения\n",
    "\n",
    "df \\\n",
    "    .drop(\"_corrupt_record\") \\\n",
    "    .distinct() \\\n",
    "    .na.drop(\"all\") \\\n",
    "    .na.fill( {'continent': 'Earth', 'population': 0 } ) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|    Earth|  Spain|   Madrid|         0|\n",
      "|   Europe|  Spain|Barselona|         0|\n",
      "|   Europe| France|    Paris|   2196936|\n",
      "|   Europe|Germany|   Berlin|   3490105|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|   Europe| Russia|   Moscow|  12380664|\n",
      "+---------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Пример выборки нескольких колонок\n",
    "\n",
    "clean_data = df \\\n",
    "    .drop(\"_corrupt_record\") \\\n",
    "    .distinct() \\\n",
    "    .na.drop(\"all\") \\\n",
    "    .na.fill( {'continent': 'Earth', 'population': 0 } ) \\\n",
    "    .select(\"continent\", \"country\", \"name\", \"population\") \\\n",
    "\n",
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|continent|count|\n",
      "+---------+-----+\n",
      "|Europe   |4    |\n",
      "|Africa   |1    |\n",
      "|Earth    |1    |\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Строим базовую группировку\n",
    "\n",
    "clean_data.groupBy('continent').count().show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|continent|count(1)|\n",
      "+---------+--------+\n",
      "|   Europe|       4|\n",
      "|   Africa|       1|\n",
      "|    Earth|       1|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Метод count можно спрятать внутри agg()\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "agg = clean_data.groupBy('continent').agg(count(\"*\"))\n",
    "\n",
    "agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|continent|count|\n",
      "+---------+-----+\n",
      "|   Europe|    4|\n",
      "|   Africa|    1|\n",
      "|    Earth|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Чтобы колонки имели правильное имя, следует использовать метод alias()\n",
    "\n",
    "agg = clean_data.groupBy('continent').agg(count(\"*\").alias(\"count\"))\n",
    "agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------+\n",
      "|continent|city_count|population_sum|\n",
      "+---------+----------+--------------+\n",
      "|   Europe|         4|      18067705|\n",
      "|   Africa|         1|      11922948|\n",
      "|    Earth|         1|             0|\n",
      "+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Добавим в группировку сумму населения на каждом континенте\n",
    "\n",
    "from pyspark.sql.functions import count, sum\n",
    "\n",
    "agg = \\\n",
    "    clean_data \\\n",
    "    .groupBy('continent') \\\n",
    "    .agg(count(\"*\").alias(\"city_count\"), sum('population').alias(\"population_sum\"))\n",
    "\n",
    "\n",
    "agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запись данных\n",
    "Основной метод записи в любые системы\n",
    "\n",
    "```df.write.format(datasource_type).options(datasource_options).mode(savemode).save(object_name)```\n",
    "\n",
    "+ ```datasource_type``` - тип источника (\"parquet\", \"json\", \"cassandra\") и т. д.\n",
    "+ ```datasource_options``` - опции для работы с источником (логины, пароли, адреса для подключения и т. д.)\n",
    "+ ```savemode``` - режим записи данных (добавление, перезапись и т. д.)\n",
    "+ ```object_name``` - имя таблицы/файла/топика/индекса\n",
    "\n",
    "[DataFrameWriter](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter):\n",
    "+ метод ```save``` является действием\n",
    "+ позволяет работать с партиционированными данными (parquet, orc)\n",
    "+ не всегда валидирует схему и формат данных\n",
    "\n",
    "\n",
    "### Список (неполный) поддерживаемых источников данных\n",
    "+ Файлы:\n",
    "  - json\n",
    "  - text\n",
    "  - csv\n",
    "  - orc\n",
    "  - parquet\n",
    "+ Базы данных\n",
    "  - elasticsearch\n",
    "  - cassandra\n",
    "  - jdbc\n",
    "  - hive\n",
    "+ Брокеры сообщений\n",
    "  - kafka\n",
    "  \n",
    "\n",
    "**Библиотеки для работы с источниками должны быть доступны в JAVA CLASSPATH на драйвере и воркерах!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok! Data is written to hdfs:///user/atitov/agg0.parquet\n"
     ]
    }
   ],
   "source": [
    "# Сохраним данные в parquet, предварительно отфильтровав данные\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "condition = col(\"continent\") != \"Earth\"\n",
    "\n",
    "agg \\\n",
    "    .filter(condition) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_parquet_agg)\n",
    "\n",
    "print(\"Ok! Data is written to {}\".format(output_parquet_agg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'>\n",
      "<class 'function'>\n"
     ]
    }
   ],
   "source": [
    "# P.S.\n",
    "# Когда мы делаем .filter в DataFrame API, мы передаем условие типа pyspark.sql.column.Column.\n",
    "print(type(condition))\n",
    "\n",
    "# когда раньше мы использовали лямбда функции в RDD, мы передавали лямбда функцию:\n",
    "condition_old = lambda x: x != \"Earth\"\n",
    "print(type(condition_old))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соединения\n",
    "\n",
    "Join'ы позволяют соединять два DF в один по заданным условиям.\n",
    "\n",
    "По типу условия join'ы делятся на:\n",
    "+ equ-join - соединение по равенству одного или более ключей\n",
    "+ non-equ join - соединение по условию, отличному от равенства одного или более ключей\n",
    "\n",
    "По методу соединения join'ы бывают:\n",
    "![Joins](http://kirillpavlov.com/images/join-types.png)\n",
    "[Источник](http://kirillpavlov.com/blog/2016/04/23/beyond-traditional-join-with-apache-spark/)\n",
    "\n",
    "При выполнении join Spark автоматически выбирает один [из доступных алгоритмов](https://youtu.be/fp53QhSfQcI) соединения и не всегда делает это оптимально, часто применяя cross join. Поэтому, в последних версиях Spark метод ```join()``` приведет к ошибке, если под капотом он будет использовать cross join. Отключить эту проверку можно с помощью опции ```--conf spark.sql.crossJoin.enabled=true```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = false)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = false)\n",
      "\n",
      "root\n",
      " |-- continent: string (nullable = true)\n",
      " |-- city_count: long (nullable = true)\n",
      " |-- population_sum: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Для демонстрации работы join используем подгтовленные данные\n",
    "clean_data.printSchema()\n",
    "\n",
    "agg = spark.read.parquet(output_parquet_agg)\n",
    "agg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = false)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = false)\n",
      " |-- city_count: long (nullable = true)\n",
      " |-- population_sum: long (nullable = true)\n",
      "\n",
      "+---------+-------+---------+----------+----------+--------------+\n",
      "|continent|country|     name|population|city_count|population_sum|\n",
      "+---------+-------+---------+----------+----------+--------------+\n",
      "|   Europe| Russia|   Moscow|  12380664|         4|      18067705|\n",
      "|   Europe|Germany|   Berlin|   3490105|         4|      18067705|\n",
      "|   Europe| France|    Paris|   2196936|         4|      18067705|\n",
      "|   Europe|  Spain|Barselona|         0|         4|      18067705|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|         1|      11922948|\n",
      "+---------+-------+---------+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Самый простой join - inner join по равенству одной колонки\n",
    "joined = clean_data.join(agg, 'continent', 'inner')\n",
    "\n",
    "joined.printSchema()\n",
    "\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = false)\n",
      " |-- x: string (nullable = false)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = false)\n",
      " |-- city_count: long (nullable = true)\n",
      " |-- population_sum: long (nullable = true)\n",
      "\n",
      "+---------+---+-------+---------+----------+----------+--------------+\n",
      "|continent|  x|country|     name|population|city_count|population_sum|\n",
      "+---------+---+-------+---------+----------+----------+--------------+\n",
      "|   Europe|  x| Russia|   Moscow|  12380664|         4|      18067705|\n",
      "|   Europe|  x|Germany|   Berlin|   3490105|         4|      18067705|\n",
      "|   Europe|  x| France|    Paris|   2196936|         4|      18067705|\n",
      "|   Europe|  x|  Spain|Barselona|         0|         4|      18067705|\n",
      "|   Africa|  x|  Egypt|    Cairo|  11922948|         1|      11922948|\n",
      "+---------+---+-------+---------+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inner join по равенству двух колонок. Поскольку двух одинаковых колонок у нас нет, мы создадим их из константы\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "left = clean_data.withColumn(\"x\", lit(\"x\"))\n",
    "right = agg.withColumn(\"x\", lit(\"x\"))\n",
    "\n",
    "joined = left.join(right, ['continent', 'x'], 'inner')\n",
    "\n",
    "joined.printSchema()\n",
    "\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent_left: string (nullable = false)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = false)\n",
      " |-- city_count_max: integer (nullable = false)\n",
      " |-- continent_right: string (nullable = true)\n",
      " |-- city_count: long (nullable = true)\n",
      " |-- population_sum: long (nullable = true)\n",
      "\n",
      "+--------------+-------+---------+----------+--------------+---------------+----------+--------------+\n",
      "|continent_left|country|     name|population|city_count_max|continent_right|city_count|population_sum|\n",
      "+--------------+-------+---------+----------+--------------+---------------+----------+--------------+\n",
      "|         Earth|  Spain|   Madrid|         0|             2|           null|      null|          null|\n",
      "|        Europe|  Spain|Barselona|         0|             2|           null|      null|          null|\n",
      "|        Europe| France|    Paris|   2196936|             2|           null|      null|          null|\n",
      "|        Europe|Germany|   Berlin|   3490105|             2|           null|      null|          null|\n",
      "|        Africa|  Egypt|    Cairo|  11922948|             2|         Africa|         1|      11922948|\n",
      "|        Europe| Russia|   Moscow|  12380664|             2|           null|      null|          null|\n",
      "+--------------+-------+---------+----------+--------------+---------------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# non-equ left join\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "left = clean_data.withColumn(\"city_count_max\", lit(2)).withColumnRenamed(\"continent\", \"continent_left\")\n",
    "right = agg.withColumnRenamed(\"continent\", \"continent_right\")\n",
    "\n",
    "join_condition = (col(\"continent_left\") == col(\"continent_right\")) & (col(\"city_count\") < col(\"city_count_max\"))\n",
    "\n",
    "joined = left.join(right, join_condition, 'left')\n",
    "\n",
    "joined.printSchema()\n",
    "\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent_left: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = true)\n",
      " |-- city_count_max: integer (nullable = true)\n",
      " |-- continent_right: string (nullable = true)\n",
      " |-- city_count: long (nullable = true)\n",
      " |-- population_sum: long (nullable = true)\n",
      "\n",
      "+--------------+-------+-----+----------+--------------+---------------+----------+--------------+\n",
      "|continent_left|country| name|population|city_count_max|continent_right|city_count|population_sum|\n",
      "+--------------+-------+-----+----------+--------------+---------------+----------+--------------+\n",
      "|          null|   null| null|      null|          null|         Europe|         4|      18067705|\n",
      "|        Africa|  Egypt|Cairo|  11922948|             2|         Africa|         1|      11922948|\n",
      "+--------------+-------+-----+----------+--------------+---------------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# non-equ right join\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "left = clean_data.withColumn(\"city_count_max\", lit(2)).withColumnRenamed(\"continent\", \"continent_left\")\n",
    "right = agg.withColumnRenamed(\"continent\", \"continent_right\")\n",
    "\n",
    "join_condition = (col(\"continent_left\") == col(\"continent_right\")) & (col(\"city_count\") < col(\"city_count_max\"))\n",
    "\n",
    "joined = left.join(right, join_condition, 'right')\n",
    "\n",
    "joined.printSchema()\n",
    "\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+---------+----------+--------------+\n",
      "|continent|country|name     |population|continent|city_count|population_sum|\n",
      "+---------+-------+---------+----------+---------+----------+--------------+\n",
      "|Earth    |Spain  |Madrid   |0         |Europe   |4         |18067705      |\n",
      "|Europe   |Spain  |Barselona|0         |Europe   |4         |18067705      |\n",
      "|Europe   |France |Paris    |2196936   |Europe   |4         |18067705      |\n",
      "|Europe   |Germany|Berlin   |3490105   |Europe   |4         |18067705      |\n",
      "|Africa   |Egypt  |Cairo    |11922948  |Europe   |4         |18067705      |\n",
      "|Europe   |Russia |Moscow   |12380664  |Europe   |4         |18067705      |\n",
      "|Earth    |Spain  |Madrid   |0         |Africa   |1         |11922948      |\n",
      "|Europe   |Spain  |Barselona|0         |Africa   |1         |11922948      |\n",
      "|Europe   |France |Paris    |2196936   |Africa   |1         |11922948      |\n",
      "|Europe   |Germany|Berlin   |3490105   |Africa   |1         |11922948      |\n",
      "|Africa   |Egypt  |Cairo    |11922948  |Africa   |1         |11922948      |\n",
      "|Europe   |Russia |Moscow   |12380664  |Africa   |1         |11922948      |\n",
      "+---------+-------+---------+----------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cross join\n",
    "clean_data.crossJoin(agg).show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = false)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = false)\n",
      " |-- city_count: long (nullable = true)\n",
      " |-- population_sum: long (nullable = true)\n",
      "\n",
      "+---------+-------+---------+----------+----------+--------------+\n",
      "|continent|country|     name|population|city_count|population_sum|\n",
      "+---------+-------+---------+----------+----------+--------------+\n",
      "|   Europe|  Spain|Barselona|         0|         4|      18067705|\n",
      "|   Europe| France|    Paris|   2196936|         4|      18067705|\n",
      "|   Europe|Germany|   Berlin|   3490105|         4|      18067705|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|         1|      11922948|\n",
      "|   Europe| Russia|   Moscow|  12380664|         4|      18067705|\n",
      "+---------+-------+---------+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Один из самых простов вариантов ускорить работу join - сделать broadcast.\n",
    "# При этом DF будет целиком склонирован на каждый воркер, что минимизирует shuffle во время выполнения join'а\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joined = clean_data.join(broadcast(agg), 'continent', 'inner')\n",
    "\n",
    "joined.printSchema()\n",
    "\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оконные функции\n",
    "\n",
    "Оконные функции позволяют делать функции над \"окнами\" (кто бы мог подумать) данных\n",
    "\n",
    "Окно создается из класса [pyspark.sql.Window](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Window) с указанием полей, определяющих границы окон и полей, определяющих порядок сортировки внутри окна:\n",
    "\n",
    "```window = Window.partitionBy(\"a\", \"b\").orderBy(\"a\")```\n",
    "\n",
    "Применяя окна, можно использовать такие полезные функции из [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions), как ```lag()``` и ```lead()```, а также эффективно работать с данными time-series, вычисляя такие параметры, как, например, среднее значение заданного поля за 3-х часовой интервал"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+----------+--------------+\n",
      "|continent|country|     name|population|city_count|population_sum|\n",
      "+---------+-------+---------+----------+----------+--------------+\n",
      "|   Europe|  Spain|Barselona|         0|         4|      18067705|\n",
      "|   Europe| France|    Paris|   2196936|         4|      18067705|\n",
      "|   Europe|Germany|   Berlin|   3490105|         4|      18067705|\n",
      "|   Europe| Russia|   Moscow|  12380664|         4|      18067705|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|         1|      11922948|\n",
      "|    Earth|  Spain|   Madrid|         0|         1|             0|\n",
      "+---------+-------+---------+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# В нашем случае, используя оконные функции, мы можем построить DF из предыдущих примеров c join, \n",
    "# но без использования соединения\n",
    "\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "window = Window.partitionBy(\"continent\")\n",
    "\n",
    "agg = clean_data \\\n",
    "    .withColumn(\"city_count\", F.count(\"*\").over(window)) \\\n",
    "    .withColumn(\"population_sum\", F.sum(\"population\").over(window)) \\\n",
    "\n",
    "agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции pyspark.sql.functions\n",
    "\n",
    "Spark обладает достаточно большим набором встроенных функций, доступных в [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions), поэтому перед тем, как писать свою UDF, стоит обязательно поискать нужную функцию в данном пакете.\n",
    "\n",
    "К тому же, все функции Spark принимают на вход и возвращают [pyspark.sql.Column](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column), а это значит, что вы можете совмещать функции вместе\n",
    "\n",
    "**Также важно помнить, что функции и колонки в Spark могут быть созданы без привязки к конкретным данным и DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+----------+--------------+--------------------+\n",
      "|continent|country|     name|population|city_count|population_sum|             avg_pop|\n",
      "+---------+-------+---------+----------+----------+--------------+--------------------+\n",
      "|   Europe|  Spain|Barselona|         0|         4|      18067705|{\"value\":4516926.25}|\n",
      "|   Europe| France|    Paris|   2196936|         4|      18067705|{\"value\":4516926.25}|\n",
      "|   Europe|Germany|   Berlin|   3490105|         4|      18067705|{\"value\":4516926.25}|\n",
      "|   Europe| Russia|   Moscow|  12380664|         4|      18067705|{\"value\":4516926.25}|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|         1|      11922948|{\"value\":1.192294...|\n",
      "|    Earth|  Spain|   Madrid|         0|         1|             0|       {\"value\":0.0}|\n",
      "+---------+-------+---------+----------+----------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "avg_pop = \\\n",
    "    to_json(\n",
    "        struct(\n",
    "            (col(\"population_sum\") / col(\"city_count\")).alias(\"value\")\n",
    "        )\n",
    "    ).alias(\"avg_pop\")\n",
    "\n",
    "agg.select(col(\"*\"), avg_pop).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- allinone: struct (nullable = false)\n",
      " |    |-- continent: string (nullable = false)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- population: long (nullable = false)\n",
      " |    |-- city_count: long (nullable = false)\n",
      " |    |-- population_sum: long (nullable = true)\n",
      "\n",
      "+-----------------------------------------------+\n",
      "|allinone                                       |\n",
      "+-----------------------------------------------+\n",
      "|[Europe, Spain, Barselona, 0, 4, 18067705]     |\n",
      "|[Europe, France, Paris, 2196936, 4, 18067705]  |\n",
      "|[Europe, Germany, Berlin, 3490105, 4, 18067705]|\n",
      "|[Europe, Russia, Moscow, 12380664, 4, 18067705]|\n",
      "|[Africa, Egypt, Cairo, 11922948, 1, 11922948]  |\n",
      "|[Earth, Spain, Madrid, 0, 1, 0]                |\n",
      "+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Большим преимуществом Spark по сравнению с большинством SQL ориентированных БД является наличие\n",
    "# встроенных функций работы со списками, словарями и структурами данных\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "all_in_one = agg.select(struct(*agg.columns).alias(\"allinone\"))\n",
    "\n",
    "all_in_one.printSchema()\n",
    "all_in_one.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|c   |\n",
      "+----+\n",
      "|true|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Например, можно создавать массивы и искать в них данные\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "arrays = \\\n",
    "    spark.range(0,1) \\\n",
    "    .withColumn(\"a\", array(lit(1), lit(2), lit(3))) \\\n",
    "    .select(array_contains(col(\"a\"), 1).alias(\"c\"))\n",
    "\n",
    "\n",
    "arrays.show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также, в разделе [SQL, Built-in Functions](https://spark.apache.org/docs/latest/api/sql/index.html) присутствует еще более широкий список функций, доступных в Spark. Некоторые из них отсутствуют в [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)! \n",
    "\n",
    "Эти функции нельзя использовать как обычные методы над [pyspark.sql.Column](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column), однако вы можете использовать метод ```expr()``` для этого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------+\n",
      "|id |a                                   |\n",
      "+---+------------------------------------+\n",
      "|0  |7afcc18f-61c4-416a-9cfe-cb9a174d0c87|\n",
      "+---+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# В данном примере мы используем Java функцию с помощью функции java_method\n",
    "# Запомните этот пример и используйте всегда, когда вам не хватает какой-либо функции в pyspark, \n",
    "# доступной в Java, ведь, используя такой подход, вы не снижаете производительность вашей программы за счет\n",
    "# передачи данных между Python и JVM приложением Spark, и при этом вам не нужно уметь писать код на Java/Scala :)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark.range(0,1).withColumn(\"a\", expr(\"java_method('java.util.UUID', 'randomUUID')\")).show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF функции\n",
    "\n",
    "В том случае, когда вам не хватает функционала, описанного выше, и требуется создать специфичную функцию, вы можете использовать UDF. \n",
    "\n",
    "в pyspark UDF бывают трех видов:\n",
    "+ native pyspark udf\n",
    "+ pandas udf\n",
    "+ java udf\n",
    "\n",
    "### native pyspark udf\n",
    "+ Низкая скорость из-за накладных расходов, связанных с передачей данных между Python и JVM\n",
    "+ По своей сути мало чем отличаются от RDD в pyspark\n",
    "+ Использование данных функций следует избегать всегда, когда это возможно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|slen(a)|\n",
      "+-------+\n",
      "|      3|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf, lit, expr\n",
    "slen = udf(lambda s: len(s), IntegerType())\n",
    "\n",
    "_ = spark.udf.register(\"slen\", slen)\n",
    "\n",
    "udf_data = \\\n",
    "    spark.range(0,1) \\\n",
    "    .select(lit(\"aaa\").alias(\"a\")) \\\n",
    "    .select(expr(\"slen(a)\"))\n",
    "\n",
    "udf_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas udf\n",
    "+ Средняя скорость работы из-за частично решенной, но все еще присутсвующей проблемы с сериализацией\n",
    "+ Называется pandas, т.к. на вход функции подаются не скалярные данные, как в обычной функции, а ```pandas.Series``` вектора\n",
    "+ Для использования данных функций необходимо включить опцию ```--conf spark.sql.execution.arrow.enabled=true```, т.к. это позволяет существенно сократить время, требуемое для копирования массивов данных в RAM между разными структурами\n",
    "+ Если вам необходимо писать UDF на python, то, на текущий момент, pandas_udf - это единственный способ, как это сделать\n",
    "+ Слабая поддержка вложенных структур и массивов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|multiply_func(id, id)|\n",
      "+---------------------+\n",
      "|                    0|\n",
      "|                    2|\n",
      "|                    4|\n",
      "|                    6|\n",
      "|                    8|\n",
      "|                   10|\n",
      "|                   12|\n",
      "|                   14|\n",
      "|                   16|\n",
      "|                   18|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def multiply_func(a, b):\n",
    "    return a + b\n",
    "\n",
    "multiply = pandas_udf(multiply_func, returnType=IntegerType())\n",
    "\n",
    "spark.range(0,10).select(multiply(col(\"id\"), col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|UDF:starts_with_e(id)|\n",
      "+---------------------+\n",
      "|                 true|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, expr\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "spark.udf.registerJavaFunction(\"starts_with_e\", \"local.spark.udf.TestUDF\", BooleanType())\n",
    "\n",
    "spark.range(0,1).withColumn(\"id\", lit(\"Egypt\")).select(expr(\"starts_with_e(id)\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
