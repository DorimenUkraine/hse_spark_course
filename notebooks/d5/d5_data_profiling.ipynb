{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Spark Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark import HiveContext\n",
    "from pyspark import SQLContext\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as f_\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import lpad\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import desc, asc\n",
    " \n",
    "os.environ['JAVA_HOME'] = '/usr/java/jdk1.8.0_171-amd64/jre'\n",
    "os.environ['SPARK_HOME'] = '/opt/cloudera/parcels/SPARK2/lib/spark2'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/opt/python/virtualenv/jupyter/lib'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/cloudera/parcels/PYENV.ZNO20008661/bin/python'\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/cloudera/parcels/PYENV.ZNO20008661/bin/python'\n",
    "\n",
    "sys.path.insert(0, os.path.join ('/opt/cloudera/parcels/SPARK2/lib/spark2', 'python'))\n",
    " \n",
    "sys.path.insert(0, os.path.join ('/opt/cloudera/parcels/SPARK2/lib/spark2', \n",
    "                                 'python/lib/py4j-0.10.7-src.zip'))\n",
    "    \n",
    "# Spark confing  \n",
    "# formulas\n",
    "    \n",
    "allocationResourceStatus = 'false'    \n",
    "    \n",
    "# set upS) \n",
    "conf = SparkConf().setMaster(\"yarn-client\").\\\n",
    "        setAppName('stg_data_profoling').\\\n",
    "        set('spark.sql.statistics.histogram.enable', 'true').\\\n",
    "        set('spark.dynamicAllocation.enabled', allocationResourceStatus).\\\n",
    "        set('spark.dynamicAllocation.executorIdleTimeout', '1m').\\\n",
    "        set('spark.sql.broadcastTimeout', '3000').\\\n",
    "        set('spark.sql.autoBroadcastJoinThreshold', '-1').\\\n",
    "        set('spark.dynamicAllocation.minExecutors', 20).\\\n",
    "        set('spark.dynamicAllocation.maxExecutors', 100).\\\n",
    "        set('spark.yarn.executor.memoryOverhead', '1g').\\\n",
    "        set('spark.executor.cores', 10).\\\n",
    "        set('spark.driver.memory', '20g').\\\n",
    "        set('spark.executor.memory', '20g').\\\n",
    "        set('mapred.input.dir.recursive','true').\\\n",
    "        set('spark.sql.parquet.binaryAsString','true').\\\n",
    "        set('spark.sql.hive.convertMetastoreParquet','false')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сдлеать функции:\n",
    "# 0 - типы данных и размер | указать какие воспринимать, как категории! \n",
    "# 0.1 - частота данных: сколько есть данных, сколько missing / mismatched\n",
    "# 1 - распределения (медиана и среднее, понять смещение)\n",
    "# 1.1 - min, lower quantile, median, upper quantile, maximum (+ top 10 значений) \n",
    "# 1.2 сделать статистики (за одно и поймёшь) -> count-min-sketch, T-digest\n",
    "# 2 - время -> агрегация от часа до дня и получить количество | изменение во времение, сравнение с предыдущей датой\n",
    "# 3 - рисуем - гистограммы, бины, выбросы\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Profiller:\n",
    "    \n",
    "    def __init__(self, spark, dfORsqlString, viz=False, cols=None, ):\n",
    "        \n",
    "        from collections import Counter\n",
    "        \n",
    "        self.spark = spark\n",
    "        \n",
    "        if type(dfORsqlString) == str:\n",
    "            self.df = (self.spark\n",
    "                       .sql(dfORsqlString)\n",
    "                       .persist(pyspark.StorageLevel.MEMORY_ONLY))\n",
    "        else:\n",
    "            self.df = dfORsqlString\n",
    "        \n",
    "        self.mostFreqItems = self.df.stat.freqItems(cols=self.df.columns).persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "        \n",
    "        self.countTypes = Counter(i[1] for i in self.df.dtypes)\n",
    "        \n",
    "        self.percentile = [25, 50, 75]\n",
    "        \n",
    "        self.viz = viz\n",
    "        \n",
    "        if cols != None:\n",
    "            self.calcCols = cols\n",
    "        else:\n",
    "            self.calcCols = self.df.columns\n",
    "        \n",
    "        # todo - сделать удаление колонок вне категорий\n",
    "        \n",
    "    def _groupToDo(self):\n",
    "        \n",
    "        # разбить числовые колонки на группы,\n",
    "        # относительно этих групп и визуализировать\n",
    "        \n",
    "        pass\n",
    "     \n",
    "        \n",
    "    def getNumiricalStats(self):\n",
    "        \n",
    "        self.res_df = self.df.select(self.calcCols)\n",
    "        \n",
    "        # step 1 - basic statistic\n",
    "        main_result_df = self.res_df.describe()\n",
    "        \n",
    "        line = pyspark.sql.Row(\"summary\")\n",
    "        tmp = spark.createDataFrame([line('count_uniq')])\n",
    "\n",
    "        # st2\n",
    "        for cl in self.res_df.columns:\n",
    "            tmp = tmp.withColumn(cl,\n",
    "                                 pyspark.sql.functions.lit(self._getUniq(cl)))\n",
    "\n",
    "        main_result_df = main_result_df.union(tmp).persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "        \n",
    "        #########################\n",
    "        # добавляем персентили\n",
    "\n",
    "        line = pyspark.sql.Row(\"summary\")\n",
    "\n",
    "        fullDF = None\n",
    "\n",
    "        for p in self.percentile:\n",
    "            tmp = spark.createDataFrame([line('{}'.format(p))])\n",
    "\n",
    "            for cl in self.res_df.columns:\n",
    "\n",
    "                tmp = tmp.withColumn(cl,\n",
    "                                     pyspark.sql.functions.lit(self._getPercentiles(cl, p)))\n",
    "\n",
    "            if fullDF != None:\n",
    "                fullDF = fullDF.union(tmp)\n",
    "            else:\n",
    "                fullDF = tmp\n",
    "\n",
    "        main_result_df = main_result_df.union(fullDF).persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "      \n",
    "        \n",
    "        #########################\n",
    "        # считаем пропуски\n",
    "        not_null_col = (\n",
    "                        self.res_df.agg(*[self._fillingInColCounting(col) for col in self.res_df.columns])\n",
    "                                   .withColumn('summary', pyspark.sql.functions.lit('is_fill'))\n",
    "                        )\n",
    "            \n",
    "        null_coll = (\n",
    "                    self.res_df.agg(*[self._NAInColCounting(col) for col in self.res_df.columns])\n",
    "                          .withColumn('summary', pyspark.sql.functions.lit('is_empty'))\n",
    "                    )\n",
    "\n",
    "        tmp = (\n",
    "               not_null_col.union(null_coll)\n",
    "                           .select(['summary',] + self.calcCols)    \n",
    "              )\n",
    "        \n",
    "        main_result_df = main_result_df.union(tmp).persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "        \n",
    "        #########################\n",
    "        # считаеm % пропусков\n",
    "        \n",
    "        dif = main_result_df.filter(\"summary in ('count', 'is_fill')\")\n",
    "\n",
    "        line = pyspark.sql.Row(\"summary\")\n",
    "\n",
    "        is_fill = spark.createDataFrame([line('is_fill_prc')])\n",
    "\n",
    "        for i in dif.columns[1:]:\n",
    "\n",
    "            is_fill = is_fill.withColumn(i,\n",
    "                                 pyspark.sql.functions.lit(\n",
    "                                 dif.withColumn(i, \n",
    "                                                pyspark.sql.functions.lit(\n",
    "                                                pyspark.sql.functions.lag(i, 1).over(Window.partitionBy(i).orderBy(i))\n",
    "                                                 /\n",
    "                                                int(dif.filter(\"summary == 'count'\").select(i).collect()[0][0])\n",
    "                                 )).select(i).collect()[1][0]))\n",
    "\n",
    "\n",
    "        dif = main_result_df.filter(\"summary in ('count', 'is_empty')\")\n",
    "\n",
    "        line = pyspark.sql.Row(\"summary\")\n",
    "\n",
    "        is_empty = spark.createDataFrame([line('is_empty_prc')])\n",
    "\n",
    "        for i in dif.columns[1:]:\n",
    "\n",
    "            is_empty = (is_empty.withColumn(i,\n",
    "                                 pyspark.sql.functions.lit(\n",
    "                                 dif.withColumn(i, \n",
    "                                                pyspark.sql.functions.when(\n",
    "                                                pyspark.sql.functions.lit(\n",
    "                                                pyspark.sql.functions.lag(i, 1).over(Window.partitionBy(i).orderBy(i))\n",
    "                                                 /\n",
    "                                                int(dif.filter(\"summary == 'count'\").select(i).collect()[0][0])).isNull(), 0)\n",
    "                                                .otherwise(\n",
    "                                                pyspark.sql.functions.lit(\n",
    "                                                pyspark.sql.functions.lag(i, 1).over(Window.partitionBy(i).orderBy(i))\n",
    "                                                 /\n",
    "                                                int(dif.filter(\"summary == 'count'\").select(i).collect()[0][0]))\n",
    "                                                )                                            \n",
    "                                 ).select(i).collect()[1][0]))\n",
    "                        )\n",
    "\n",
    "    \n",
    "        main_result_df = main_result_df.union(is_fill.union(is_empty)).persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "        \n",
    "        #########################\n",
    "        # считаем пропуски пропуски\n",
    "        # наклон нашего распределения в данных\n",
    "\n",
    "        line = pyspark.sql.Row(\"summary\")\n",
    "\n",
    "        tmp = spark.createDataFrame([line('skewness'), line('kurtosis')])\n",
    "\n",
    "        for cl in self.res_df.columns:\n",
    "\n",
    "            for d in self._skewness(cl):\n",
    "\n",
    "                tmp = tmp.withColumn(cl,  pyspark.sql.functions.lit(d))\n",
    "\n",
    "        main_result_df = main_result_df.union(tmp).persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "        \n",
    "        \n",
    "        if self.viz:\n",
    "            for col in self.calcCols:\n",
    "                \n",
    "                              \n",
    "                x = self.res_df\\\n",
    "                        .select(col)\\\n",
    "                        .filter(pyspark.sql.functions.col(col).isNotNull())\\\n",
    "                        .distinct()\\\n",
    "                        .toPandas()[col].astype('int')\n",
    "                \n",
    "                self.numvericalViz(x, col)\n",
    "                self.distViz(x, col)\n",
    "        \n",
    "                self.gridPlotViz(col)\n",
    "        \n",
    "        return main_result_df    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # изменение в неисчисляемые типы (категории)\n",
    "    def _changeToCat(self, df, catList):\n",
    "\n",
    "        catPatterns = ['string',]\n",
    "\n",
    "        for i in df.dtypes:\n",
    "            if i[1] in catPatterns and i[0] not in catList:\n",
    "                catList.append(i[0])\n",
    "\n",
    "        for i in catList:\n",
    "\n",
    "            if i in df.columns:\n",
    "                df = df.withColumn('{}'.format(i),\n",
    "                                   pyspark.sql.functions.col('{}'.format(i)).cast(pyspark.sql.types.StringType()))\n",
    "            else:\n",
    "                 raise Exception(\"Name {} does not exist in columns\".format(i))\n",
    "\n",
    "        return df, [i for i in df.columns if i not in catList]\n",
    "\n",
    "\n",
    "    # не пропуски / пропуски\n",
    "    def _fillingInColCounting(self, col, nanAsNull = False):\n",
    "        pred = pyspark.sql.functions.col(col).isNotNull() & (~isnan(col) if nanAsNull else pyspark.sql.functions.lit(True))\n",
    "        return pyspark.sql.functions.sum(pred.cast(pyspark.sql.types.IntegerType())).alias(col)\n",
    "\n",
    "    def _NAInColCounting(self, col, nanAsNull = False):\n",
    "        pred = pyspark.sql.functions.col(col).isNull()\n",
    "        return pyspark.sql.functions.sum(pred.cast(pyspark.sql.types.IntegerType())).alias(col)\n",
    "    \n",
    "    # уникальных значений\n",
    "    def _getUniq(self, col):\n",
    "        return int(self.res_df.select(pyspark.sql.functions.countDistinct(col)).collect()[0][0])\n",
    "    \n",
    "    # добавим персентили \n",
    "    def _getPercentiles(self, col, p):\n",
    "        #percentile = [25, 50, 75]\n",
    "        line = pyspark.sql.Row(col)\n",
    "        perc = np.transpose(np.percentile([float(r[col]) for r in self.res_df.select(col).filter(pyspark.sql.functions.col(col).isNotNull()).collect()], p))\n",
    "        percsDF = spark.createDataFrame(line(float(perc)), pyspark.sql.types.FloatType())\n",
    "\n",
    "        return float(percsDF.collect()[0][0])\n",
    "    \n",
    "        # смещение (скос) распределения\n",
    "    def _skewness(self, col):\n",
    "        return self.res_df.filter(pyspark.sql.functions.col(col).isNotNull())\\\n",
    "                          .select(pyspark.sql.functions.skewness(col),\n",
    "                                  pyspark.sql.functions.kurtosis(col))\\\n",
    "                          .collect()[0]\n",
    "            \n",
    "    def _todo(self):\n",
    "        from pyspark.ml.stat import ChiSquareTest\n",
    "        from pyspark.ml.linalg import Vectors\n",
    "        from pyspark.ml.feature import VectorAssembler\n",
    "        # todo подготовить фичи в вектор\n",
    "        # r.pValues\n",
    "        # r.degreesOfFreedom\n",
    "        # r.statistics\n",
    "        \n",
    "        # some_feature = VectorAssembler(inputCols=[],\n",
    "                                         #outputCol=\"\")\n",
    "        # df = some_feature.transform(data)\n",
    "        \n",
    "        #r = ChiSquareTest.test(res_df(some_feature?), \"feature\", \"label\") #.head()\n",
    "\n",
    "            \n",
    "    def corrViz(self):\n",
    "        \n",
    "        line = pyspark.sql.Row(\"stats\")\n",
    "\n",
    "        fullCorrDF = None\n",
    "        for i in self.calcCols:\n",
    "            tmp = spark.createDataFrame([line(i)])\n",
    "            for j in self.calcCols:\n",
    "\n",
    "                if i == j:\n",
    "                    tmp = tmp.withColumn(j, \n",
    "                                         pyspark.sql.functions.lit(0))\n",
    "                else:\n",
    "                    tmp = tmp.withColumn(j, \n",
    "                                        pyspark.sql.functions.lit(df.df.stat.corr(col1=i, col2=j)))\n",
    "\n",
    "            if fullCorrDF != None:\n",
    "                fullCorrDF = fullCorrDF.union(tmp)\n",
    "\n",
    "            else:\n",
    "                fullCorrDF = tmp\n",
    "\n",
    "        sns.heatmap(fullCorrDF.toPandas().set_index('stats')) \n",
    "        \n",
    "        \n",
    "    def numvericalViz(self, x, col):\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        %matplotlib inline\n",
    "                \n",
    "        bins = np.arange(0, x.max(), x.max()/ 10)\n",
    "\n",
    "        ########################################################################\n",
    "        hist, bin_edges = np.histogram(x,\n",
    "                                       bins,\n",
    "                                       weights=np.zeros_like(x) + 100. / x.size)\n",
    "        \n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        ax = fig.add_subplot(1, 2, 1)\n",
    "\n",
    "        # Plot по высоте значений\n",
    "        ax.bar(range(len(hist)),hist,width=1,alpha=0.8,ec ='black', color='gold')\n",
    "        \n",
    "        # # устанавливаем метки по осям\n",
    "        ax.set_xticks([0.5+i for i,j in enumerate(hist)])\n",
    "        \n",
    "        # устанваливаем определение границ и подписм \n",
    "        labels =['{}'.format(int(bins[i+1])) for i,j in enumerate(hist)]\n",
    "        labels.insert(0,'0')\n",
    "        ax.set_xticklabels(labels)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('percentage')\n",
    "\n",
    "        ########################################################################\n",
    "        # % плот\n",
    "\n",
    "        hist, bin_edges = np.histogram(x,bins) \n",
    "        \n",
    "        \n",
    "        ax = fig.add_subplot(1, 2, 2)\n",
    "        \n",
    "        ax.bar(range(len(hist)),hist,width=1,alpha=0.8,ec ='black', color='gold')\n",
    "\n",
    "        ax.set_xticks([0.5+i for i,j in enumerate(hist)])\n",
    "\n",
    "        labels =['{}'.format(int(bins[i+1])) for i,j in enumerate(hist)]\n",
    "        labels.insert(0,'0')\n",
    "        ax.set_xticklabels(labels)\n",
    "        plt.xlabel(col)\n",
    "        \n",
    "        plt.ylabel('count')\n",
    "        plt.suptitle('Histogram of {}: Left with percentage; Right with count'\n",
    "                     .format(col), size=14)\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    \n",
    "    def distViz(self, x, col):\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        %matplotlib inline\n",
    "        import seaborn as sns\n",
    "        \n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        ax = fig.add_subplot(1, 2, 1)\n",
    "        ax = sns.boxplot(data=x)\n",
    "\n",
    "        ax = fig.add_subplot(1, 2, 2)\n",
    "        ax = sns.violinplot(data=x)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def parter(self, splits):\n",
    "        patterns = {-float('inf') : \"-\", float('inf') : \"+\"}\n",
    "\n",
    "        bins = dict()\n",
    "\n",
    "        for n, v in enumerate(splits):\n",
    "            if n == 0:\n",
    "                pass\n",
    "            else:\n",
    "                bins['{}'.format(n-1)] = \"[{}:{}]\".format(splits[n-1] if splits[n-1] not in patterns.keys() else patterns[splits[n-1]],\n",
    "                                                        splits[n] if splits[n] not in patterns.keys() else patterns[splits[n]])\n",
    "        return bins     \n",
    "        \n",
    "        \n",
    "    def gridPlotViz(self, col):\n",
    "        \n",
    "        # backets\n",
    "        import matplotlib.pyplot as plt\n",
    "        %matplotlib inline\n",
    "        import seaborn as sns\n",
    "        from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "        mx = int((self.res_df.select(pyspark.sql.functions.max(col)).collect()[0][0]))\n",
    "\n",
    "        spl = [-float('inf'),]  + list(range(0, mx, int(mx/10))) + [float('inf'),]\n",
    "        \n",
    "        bins = self.parter(spl)\n",
    "        \n",
    "        \n",
    "\n",
    "        cnt_bucketizer = Bucketizer(splits=spl, \n",
    "                                    inputCol = col,\n",
    "                                    outputCol = '{}_bucket'.format(col))\n",
    "\n",
    "        tmp = cnt_bucketizer.setHandleInvalid(\"keep\").transform(self.res_df)\n",
    "        \n",
    "        binToString  = udf(lambda x: bins[x], pyspark.sql.types.StringType())\n",
    "        \n",
    "       \n",
    "        #tmp = tmp.withColumn('{}_bucket'.format(col), binToString('{}_bucket'.format(col)))\n",
    "\n",
    "\n",
    "        sns.set(style=\"ticks\")\n",
    "\n",
    "        d_f = tmp.na.drop().toPandas().astype('int')\n",
    "        d_f['{}_bucket'.format(col)] = d_f['{}_bucket'.format(col)].apply(lambda x: bins[str(x)])\n",
    "        \n",
    "        d_f[[c for c in d_f.columns if c != '{}_bucket'.format(col)]].astype('int', inplace=True)\n",
    "        sns.pairplot(d_f, hue='{}_bucket'.format(col))\n",
    "        plt.show()\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = Profiller(spark,\n",
    "               'SELECT * FROM *',\n",
    "               True,\n",
    "              ['rnd', 'cnt_users', 'avg_cltv', 'cnt_opers'])\n",
    "\n",
    "r = df.getNumiricalStats()\n",
    "r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corrViz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mostFreqItems.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3 (ZNO20008661)",
   "language": "python",
   "name": "python35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
